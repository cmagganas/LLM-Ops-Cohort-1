{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Ops Visibility and Caching Strategies\n",
    "\n",
    "# ðŸ—ï¸ Build\n",
    "\n",
    "You will build an application that leverages a visibility tool (Weights and Biases Promopts) and prompt caching.\n",
    "\n",
    "# ðŸš¢ Ship\n",
    "\n",
    "You will ship that application to a Hugging Face space.\n",
    "\n",
    "# ðŸš€ Share\n",
    "\n",
    "Create a social media post explaning or showcasing the power of prompt-caching, and visibility tooling in your LLM Ops stack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visibility Tools\n",
    "\n",
    "A key part of LLM Ops is having a visibility platform where you can track, trace, and collect, various prompt and user data. \n",
    "\n",
    "Let's take a look at it in this notebook!\n",
    "\n",
    "As always, we'll want to start with our dependencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U \"wandb>=0.15.4\" \"langchain>=0.0.218\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting started with Weights and Biases Prompts can be as easy as setting the `LANGCHAIN_WANDB_TRACING` environment variable to `true`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_WANDB_TRACING\"] = \"true\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"langchain-testing\"\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"./wandb_notebook.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_API_KEY\"] = getpass.getpass(\"Enter your WandB API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can set up our simple application!\n",
    "\n",
    "We're going to create an agent with the following characteristics:\n",
    "\n",
    "1. `ChatOpenAI` : `gpt-3.5-turbo` powered, `temperature` set to reduce creativity\n",
    "2. `arxiv` tool\n",
    "3. `ZERO_SHOT_REACT_DESCRIPTION` agent\n",
    "\n",
    "Please refer to the following documentation if you get stuck:\n",
    "\n",
    "- [ChatOpenAI](https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.openai.ChatOpenAI.html)\n",
    "- [load_tools](https://api.python.langchain.com/en/latest/agents/langchain.agents.load_tools.load_tools.html)\n",
    "- [initialize_agent](https://api.python.langchain.com/en/latest/agents/langchain.agents.initialize.initialize_agent.html)\n",
    "- [AgentType](https://api.python.langchain.com/en/latest/agents/langchain.agents.agent_types.AgentType.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import load_tools, initialize_agent, AgentType\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model='gpt-3.5-turbo-16k',\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "tools = load_tools(\n",
    "    [\"arxiv\"],\n",
    ")\n",
    "\n",
    "agent_chain = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find ./wandb_notebook.ipynb.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Streaming LangChain activity to W&B at https://wandb.ai/cmagganas/langchain-testing/runs/f2q3m7vs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: `WandbTracer` is currently in beta.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Please report any issues to https://github.com/wandb/wandb/issues with the tag `langchain`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI'm not familiar with QLoRA, so I should search for it on arxiv.org to gather more information.\n",
      "Action: arxiv\n",
      "Action Input: QLoRA\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPublished: 2023-05-23\n",
      "Title: QLoRA: Efficient Finetuning of Quantized LLMs\n",
      "Authors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\n",
      "Summary: We present QLoRA, an efficient finetuning approach that reduces memory usage\n",
      "enough to finetune a 65B parameter model on a single 48GB GPU while preserving\n",
      "full 16-bit finetuning task performance. QLoRA backpropagates gradients through\n",
      "a frozen, 4-bit quantized pretrained language model into Low Rank\n",
      "Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\n",
      "previous openly released models on the Vicuna benchmark, reaching 99.3% of the\n",
      "performance level of ChatGPT while only requiring 24 hours of finetuning on a\n",
      "single GPU. QLoRA introduces a number of innovations to save memory without\n",
      "sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\n",
      "information theoretically optimal for normally distributed weights (b) double\n",
      "quantization to reduce the average memory footprint by quantizing the\n",
      "quantization constants, and (c) paged optimziers to manage memory spikes. We\n",
      "use QLoRA to finetune more than 1,000 models, providing a detailed analysis of\n",
      "instruction following and chatbot performance across 8 instruction datasets,\n",
      "multiple model types (LLaMA, T5), and model scales that would be infeasible to\n",
      "run with regular finetuning (e.g. 33B and 65B parameter models). Our results\n",
      "show that QLoRA finetuning on a small high-quality dataset leads to\n",
      "state-of-the-art results, even when using smaller models than the previous\n",
      "SoTA. We provide a detailed analysis of chatbot performance based on both human\n",
      "and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\n",
      "alternative to human evaluation. Furthermore, we find that current chatbot\n",
      "benchmarks are not trustworthy to accurately evaluate the performance levels of\n",
      "chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\n",
      "ChatGPT. We release all of our models and code, including CUDA kernels for\n",
      "4-bit training.\n",
      "\n",
      "Published: 2023-07-20\n",
      "Title: IvyGPT: InteractiVe Chinese pathwaY language model in medical domain\n",
      "Authors: Rongsheng Wang, Yaofei Duan, ChanTong Lam, Jiexi Chen, Jiangsheng Xu, Haoming Chen, Xiaohong Liu, Patrick Cheong-Iao Pang, Tao Tan\n",
      "Summary: General large language models (LLMs) such as ChatGPT have shown remarkable\n",
      "success. However, such LLMs have not been widely adopted for medical purposes,\n",
      "due to poor accuracy and inability to provide medical advice. We propose\n",
      "IvyGPT, an LLM based on LLaMA that is trained and fine-tuned with high-quality\n",
      "medical question-answer (QA) instances and Reinforcement Learning from Human\n",
      "Feedback (RLHF). After supervised fine-tuning, IvyGPT has good multi-turn\n",
      "conversation capabilities, but it cannot perform like a doctor in other\n",
      "aspects, such as comprehensive diagnosis. Through RLHF, IvyGPT can output\n",
      "richer diagnosis and treatment answers that are closer to human. In the\n",
      "training, we used QLoRA to train 33 billion parameters on a small number of\n",
      "NVIDIA A100 (80GB) GPUs. Experimental results show that IvyGPT has outperformed\n",
      "other medical GPT models.\n",
      "\n",
      "Published: 2023-08-20\n",
      "Title: LMTuner: An user-friendly and highly-integrable Training Framework for fine-tuning Large Language Models\n",
      "Authors: Yixuan Weng, Zhiqi Wang, Huanxuan Liao, Shizhu He, Shengping Liu, Kang Liu, Jun Zhao\n",
      "Summary: With the burgeoning development in the realm of large language models (LLMs),\n",
      "the demand for efficient incremental training tailored to specific industries\n",
      "and domains continues to increase. Currently, the predominantly employed\n",
      "frameworks lack modular design, it often takes a lot of coding work to\n",
      "kickstart the training of LLM. To address this, we present \"LMTuner\", a highly\n",
      "usable, integrable, and scalable system for training LLMs expeditiously and\n",
      "with minimal user-input. LMTuner comprises three main modules - the\n",
      "Interaction, Training, and Inference Modules. We advocate that LMTuner's\n",
      "usability and integrality alleviate the complexi\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mQLoRA is an efficient finetuning approach that reduces memory usage in pretrained language models. It backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA). QLoRA introduces several innovations to save memory without sacrificing performance, including a new data type called 4-bit NormalFloat (NF4), double quantization, and paged optimizers. It has been used to finetune more than 1,000 models and has achieved state-of-the-art results in chatbot performance. The paper also mentions the release of models and code, including CUDA kernels for 4-bit training.\n",
      "\n",
      "Final Answer: QLoRA is an efficient finetuning approach that reduces memory usage in pretrained language models and has achieved state-of-the-art results in chatbot performance.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'QLoRA is an efficient finetuning approach that reduces memory usage in pretrained language models and has achieved state-of-the-art results in chatbot performance.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_chain.run(\"What is QLoRA?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, now that we have some outputs - let's see what Weights and Biases was able to do!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring WandB Outputs\n",
    "\n",
    "First things first, we'll want to head to our WandB home page and find our projects!\n",
    "\n",
    "You'll navigate to `wandb.ai/{YOUR_USERNAME_HERE}` - and then click the `Projects` tab.\n",
    "\n",
    "![image](https://i.imgur.com/mplxa4p.png)\n",
    "\n",
    "Now we can head into our project, which should be named `langchain-testing`:\n",
    "\n",
    "![image](https://i.imgur.com/Q4AU0NC.png)\n",
    "\n",
    "Explore all the tools made available to you through the Prompt Workspace!\n",
    "\n",
    "Let's try another prompt and see what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI'm not sure what LLM Ops is. I should search for it on arxiv.org to find more information.\n",
      "Action: arxiv\n",
      "Action Input: LLM Ops\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPublished: 2018-06-18\n",
      "Title: Exciting LLM Geometries\n",
      "Authors: Robert de Mello Koch, Jia-Hui Huang, Laila Tribelhorn\n",
      "Summary: We study excitations of LLM geometries. These geometries arise from the\n",
      "backreaction of a condensate of giant gravitons. Excitations of the condensed\n",
      "branes are open strings, which give rise to an emergent Yang-Mills theory at\n",
      "low energy. We study the dynamics of the planar limit of these emergent gauge\n",
      "theories, accumulating evidence that they are planar ${\\cal N}=4$ super\n",
      "Yang-Mills. There are three observations supporting this conclusion: (i) we\n",
      "argue for an isomorphism between the planar Hilbert space of the original\n",
      "${\\cal N}=4$ super Yang-Mills and the planar Hilbert space of the emergent\n",
      "gauge theory, (ii) we argue that the OPE coefficients of the planar limit of\n",
      "the emergent gauge theory vanish and (iii) we argue that the planar spectrum of\n",
      "anomalous dimensions of the emergent gauge theory is that of planar ${\\cal\n",
      "N}=4$ super Yang-Mills. Despite the fact that the planar limit of the emergent\n",
      "gauge theory is planar ${\\cal N}=4$ super Yang-Mills, we explain why the\n",
      "emergent gauge theory is not ${\\cal N}=4$ super Yang-Mills theory.\n",
      "\n",
      "Published: 2023-07-19\n",
      "Title: On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models\n",
      "Authors: Sarah Gao, Andrew Kean Gao\n",
      "Summary: Since late 2022, Large Language Models (LLMs) have become very prominent with\n",
      "LLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\n",
      "are announced each week, many of which are deposited to Hugging Face, a\n",
      "repository of machine learning models and datasets. To date, nearly 16,000 Text\n",
      "Generation models have been uploaded to the site. Given the huge influx of\n",
      "LLMs, it is of interest to know which LLM backbones, settings, training\n",
      "methods, and families are popular or trending. However, there is no\n",
      "comprehensive index of LLMs available. We take advantage of the relatively\n",
      "systematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\n",
      "and identify communities amongst LLMs using n-grams and term frequency-inverse\n",
      "document frequency. Our methods successfully identify families of LLMs and\n",
      "accurately cluster LLMs into meaningful subgroups. We present a public web\n",
      "application to navigate and explore Constellation, our atlas of 15,821 LLMs.\n",
      "Constellation rapidly generates a variety of visualizations, namely\n",
      "dendrograms, graphs, word clouds, and scatter plots. Constellation is available\n",
      "at the following link: https://constellation.sites.stanford.edu/.\n",
      "\n",
      "Published: 2023-08-16\n",
      "Title: TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series\n",
      "Authors: Chenxi Sun, Yaliang Li, Hongyan Li, Shenda Hong\n",
      "Summary: This work summarizes two strategies for completing time-series (TS) tasks\n",
      "using today's language model (LLM): LLM-for-TS, design and train a fundamental\n",
      "large model for TS data; TS-for-LLM, enable the pre-trained LLM to handle TS\n",
      "data. Considering the insufficient data accumulation, limited resources, and\n",
      "semantic context requirements, this work focuses on TS-for-LLM methods, where\n",
      "we aim to activate LLM's ability for TS data by designing a TS embedding method\n",
      "suitable for LLM. The proposed method is named TEST. It first tokenizes TS,\n",
      "builds an encoder to embed them by instance-wise, feature-wise, and\n",
      "text-prototype-aligned contrast, and then creates prompts to make LLM more open\n",
      "to embeddings, and finally implements TS tasks. Experiments are carried out on\n",
      "TS classification and forecasting tasks using 8 LLMs with different structures\n",
      "and sizes. Although its results cannot significantly outperform the current\n",
      "SOTA models customized for TS tasks, by treating LLM as the pattern machine, it\n",
      "can endow LLM's ability to process TS data without compromising the language\n",
      "ability. This paper is intended to serve as a foundational work that will\n",
      "inspire further research.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mLLM Ops appears to have multiple meanings. One is related to \"Exciting LLM Geometries\" in the field of physics, while another is related to \"Large Language Models\" in the field of natural language processing. It's important to clarify which meaning is being referred to in the original question.\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to clarify which meaning of LLM Ops is being referred to in the original question.\n",
      "Action: None\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action Input:' after 'Action:'\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to clarify which meaning of LLM Ops is being referred to in the original question.\n",
      "Action: None\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action Input:' after 'Action:'\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: The meaning of LLM Ops is ambiguous and needs to be clarified. It could refer to \"Exciting LLM Geometries\" in the field of physics or \"Large Language Models\" in the field of natural language processing.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The meaning of LLM Ops is ambiguous and needs to be clarified. It could refer to \"Exciting LLM Geometries\" in the field of physics or \"Large Language Models\" in the field of natural language processing.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_chain.run(\"What is LLM Ops?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, repeated calls will continue to add more information to our `langchain-testing` project!\n",
    "\n",
    "![image](https://i.imgur.com/Xze6jNE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a slightly more complex application by adding a Prompt Cache!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding A Prompt Cache\n",
    "\n",
    "The basic idea of Prompt Caching is to provide a way to circumvent going to the LLM for prompts we have already seen.\n",
    "\n",
    "Similar to cached embeddings, the idea is simple:\n",
    "\n",
    "- Keep track of all the input/output pairs\n",
    "- If a user query is (in the case of semantic similarity caches) close enough to a previous prompt contained in the cache, return the output associated with that pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing a Prompt Cache\n",
    "\n",
    "There are many different tools you can use to implement a Prompt Cache - from a \"build it yourself\" VectorStore implementation - to Redis - to custom libraries - there are upsides and downsides to each solution. \n",
    "\n",
    "Let's look at the Redis-backed Cache vs. `InMemoryCache` as an example:\n",
    "\n",
    "Redis Cache\n",
    "\n",
    "| Pros  | Cons  |\n",
    "|---|---|\n",
    "| Managed and Robust  | Expensive to Host  |\n",
    "| Integrations on all Major Cloud Platforms  | Non-trivial to Integrate |\n",
    "| Easily Scalable  | Does not have a ChatModel implementation |\n",
    "\n",
    "`InMemoryCache`\n",
    "\n",
    "| Pros  | Cons  |\n",
    "|---|---|\n",
    "| Easily implemented  | Consumes potentially precious memory |\n",
    "| Completely Cloud Agnostic  | Does not offer inter-session caching |\n",
    "\n",
    "For the sake of ease of use - and to allow functionality with our `ChatOpenAI` model - we'll leverage `InMemoryCache`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to set our `langchain.llm_cache` to use the `InMemoryCache`.\n",
    "\n",
    "- [`InMemoryCache`](https://api.python.langchain.com/en/latest/cache/langchain.cache.InMemoryCache.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain.cache import InMemoryCache\n",
    "\n",
    "langchain.llm_cache = InMemoryCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more important fact about the `InMemoryCache` is that it is what's called an \"exact-match\" cache - meaning it will only trigger when the user query is *exactly* represented in the cache. \n",
    "\n",
    "This is a safer cache, as we can guarentee the user's query exactly matches with previous queries and we don't have to worry about edge-cases where semantic similarity might fail - but it does reduce the potential to hit the cache.\n",
    "\n",
    "We could leverage tools like `GPTCache`, or `RedisCache` (for non-chat model implementations) to get a \"semantic similarity\" cache, if desired!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use the arxiv tool to search for articles on \"Retrieval Augmented Generation\" to find a definition or explanation.\n",
      "Action: arxiv\n",
      "Action Input: \"Retrieval Augmented Generation\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPublished: 2022-02-13\n",
      "Title: A Survey on Retrieval-Augmented Text Generation\n",
      "Authors: Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu\n",
      "Summary: Recently, retrieval-augmented text generation attracted increasing attention\n",
      "of the computational linguistics community. Compared with conventional\n",
      "generation models, retrieval-augmented text generation has remarkable\n",
      "advantages and particularly has achieved state-of-the-art performance in many\n",
      "NLP tasks. This paper aims to conduct a survey about retrieval-augmented text\n",
      "generation. It firstly highlights the generic paradigm of retrieval-augmented\n",
      "generation, and then it reviews notable approaches according to different tasks\n",
      "including dialogue response generation, machine translation, and other\n",
      "generation tasks. Finally, it points out some important directions on top of\n",
      "recent methods to facilitate future research.\n",
      "\n",
      "Published: 2023-05-11\n",
      "Title: Active Retrieval Augmented Generation\n",
      "Authors: Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig\n",
      "Summary: Despite the remarkable ability of large language models (LMs) to comprehend\n",
      "and generate language, they have a tendency to hallucinate and create factually\n",
      "inaccurate output. Augmenting LMs by retrieving information from external\n",
      "knowledge resources is one promising solution. Most existing\n",
      "retrieval-augmented LMs employ a retrieve-and-generate setup that only\n",
      "retrieves information once based on the input. This is limiting, however, in\n",
      "more general scenarios involving generation of long texts, where continually\n",
      "gathering information throughout the generation process is essential. There\n",
      "have been some past efforts to retrieve information multiple times while\n",
      "generating outputs, which mostly retrieve documents at fixed intervals using\n",
      "the previous context as queries. In this work, we provide a generalized view of\n",
      "active retrieval augmented generation, methods that actively decide when and\n",
      "what to retrieve across the course of the generation. We propose\n",
      "Forward-Looking Active REtrieval augmented generation (FLARE), a generic\n",
      "retrieval-augmented generation method which iteratively uses a prediction of\n",
      "the upcoming sentence to anticipate future content, which is then utilized as a\n",
      "query to retrieve relevant documents to regenerate the sentence if it contains\n",
      "low-confidence tokens. We test FLARE along with baselines comprehensively over\n",
      "4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\n",
      "superior or competitive performance on all tasks, demonstrating the\n",
      "effectiveness of our method. Code and datasets are available at\n",
      "https://github.com/jzbjyb/FLARE.\n",
      "\n",
      "Published: 2023-08-08\n",
      "Title: Hybrid Retrieval-Augmented Generation for Real-time Composition Assistance\n",
      "Authors: Xuchao Zhang, Menglin Xia, Camille Couturier, Guoqing Zheng, Saravan Rajmohan, Victor Ruhle\n",
      "Summary: Retrieval augmented models show promise in enhancing traditional language\n",
      "models by improving their contextual understanding, integrating private data,\n",
      "and reducing hallucination. However, the processing time required for retrieval\n",
      "augmented large language models poses a challenge when applying them to tasks\n",
      "that require real-time responses, such as composition assistance.\n",
      "  To overcome this limitation, we propose the Hybrid Retrieval-Augmented\n",
      "Generation (HybridRAG) framework that leverages a hybrid setting that combines\n",
      "both client and cloud models. HybridRAG incorporates retrieval-augmented memory\n",
      "generated asynchronously by a Large Language Model (LLM) in the cloud. By\n",
      "integrating this retrieval augmented memory, the client model acquires the\n",
      "capability to generate highly effective responses, benefiting from the LLM's\n",
      "capabilities. Furthermore, through asynchronous memory integration, the client\n",
      "model is capable of delivering real-time responses to user requests without the\n",
      "need to wait for memory synchronization from the cloud. Our experiments on\n",
      "Wikitext and Pile subsets show that HybridRAG \u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mBased on the search results, \"Retrieval Augmented Generation\" refers to a technique that combines retrieval of information from external knowledge resources with text generation models. It has been applied in various natural language processing tasks, including dialogue response generation, machine translation, and composition assistance. The technique aims to enhance the contextual understanding of language models, reduce hallucination, and improve the quality of generated text. There are different approaches and methods proposed in the literature, such as retrieve-and-generate setups and active retrieval augmented generation. The effectiveness of retrieval augmented generation has been demonstrated in several studies, achieving state-of-the-art performance in various tasks.\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3mI should provide the final answer to the original input question.\n",
      "Final Answer: Retrieval Augmented Generation refers to a technique that combines retrieval of information from external knowledge resources with text generation models. It has been applied in various natural language processing tasks, including dialogue response generation, machine translation, and composition assistance. The technique aims to enhance the contextual understanding of language models, reduce hallucination, and improve the quality of generated text. There are different approaches and methods proposed in the literature, such as retrieve-and-generate setups and active retrieval augmented generation. The effectiveness of retrieval augmented generation has been demonstrated in several studies, achieving state-of-the-art performance in various tasks.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "CPU times: user 196 ms, sys: 19.1 ms, total: 215 ms\n",
      "Wall time: 11.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Retrieval Augmented Generation refers to a technique that combines retrieval of information from external knowledge resources with text generation models. It has been applied in various natural language processing tasks, including dialogue response generation, machine translation, and composition assistance. The technique aims to enhance the contextual understanding of language models, reduce hallucination, and improve the quality of generated text. There are different approaches and methods proposed in the literature, such as retrieve-and-generate setups and active retrieval augmented generation. The effectiveness of retrieval augmented generation has been demonstrated in several studies, achieving state-of-the-art performance in various tasks.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "agent_chain.run(\"What is Retrieval Augmented Generation?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that's great! Working as expected - let's take a look at the output of our `ChatOpenAI` module in our Weights and Biases project:\n",
    "\n",
    "```\n",
    "{ \"token_usage\": { \"prompt_tokens\": 1057, \"completion_tokens\": 130, \"total_tokens\": 1187 }, \"model_name\": \"gpt-3.5-turbo-0613\" }\n",
    "```\n",
    "\n",
    "So, you can see: We used `1187` total tokens, and the request took ~8s.\n",
    "\n",
    "Let's look at the full output of our Weights and Biases project:\n",
    "\n",
    "![image](https://i.imgur.com/cU8NuDK.png)\n",
    "\n",
    "Let's try the same request again and see what happens this time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use the arxiv tool to search for articles on \"Retrieval Augmented Generation\" to find a definition or explanation.\n",
      "Action: arxiv\n",
      "Action Input: \"Retrieval Augmented Generation\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPublished: 2022-02-13\n",
      "Title: A Survey on Retrieval-Augmented Text Generation\n",
      "Authors: Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu\n",
      "Summary: Recently, retrieval-augmented text generation attracted increasing attention\n",
      "of the computational linguistics community. Compared with conventional\n",
      "generation models, retrieval-augmented text generation has remarkable\n",
      "advantages and particularly has achieved state-of-the-art performance in many\n",
      "NLP tasks. This paper aims to conduct a survey about retrieval-augmented text\n",
      "generation. It firstly highlights the generic paradigm of retrieval-augmented\n",
      "generation, and then it reviews notable approaches according to different tasks\n",
      "including dialogue response generation, machine translation, and other\n",
      "generation tasks. Finally, it points out some important directions on top of\n",
      "recent methods to facilitate future research.\n",
      "\n",
      "Published: 2023-05-11\n",
      "Title: Active Retrieval Augmented Generation\n",
      "Authors: Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig\n",
      "Summary: Despite the remarkable ability of large language models (LMs) to comprehend\n",
      "and generate language, they have a tendency to hallucinate and create factually\n",
      "inaccurate output. Augmenting LMs by retrieving information from external\n",
      "knowledge resources is one promising solution. Most existing\n",
      "retrieval-augmented LMs employ a retrieve-and-generate setup that only\n",
      "retrieves information once based on the input. This is limiting, however, in\n",
      "more general scenarios involving generation of long texts, where continually\n",
      "gathering information throughout the generation process is essential. There\n",
      "have been some past efforts to retrieve information multiple times while\n",
      "generating outputs, which mostly retrieve documents at fixed intervals using\n",
      "the previous context as queries. In this work, we provide a generalized view of\n",
      "active retrieval augmented generation, methods that actively decide when and\n",
      "what to retrieve across the course of the generation. We propose\n",
      "Forward-Looking Active REtrieval augmented generation (FLARE), a generic\n",
      "retrieval-augmented generation method which iteratively uses a prediction of\n",
      "the upcoming sentence to anticipate future content, which is then utilized as a\n",
      "query to retrieve relevant documents to regenerate the sentence if it contains\n",
      "low-confidence tokens. We test FLARE along with baselines comprehensively over\n",
      "4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves\n",
      "superior or competitive performance on all tasks, demonstrating the\n",
      "effectiveness of our method. Code and datasets are available at\n",
      "https://github.com/jzbjyb/FLARE.\n",
      "\n",
      "Published: 2023-08-08\n",
      "Title: Hybrid Retrieval-Augmented Generation for Real-time Composition Assistance\n",
      "Authors: Xuchao Zhang, Menglin Xia, Camille Couturier, Guoqing Zheng, Saravan Rajmohan, Victor Ruhle\n",
      "Summary: Retrieval augmented models show promise in enhancing traditional language\n",
      "models by improving their contextual understanding, integrating private data,\n",
      "and reducing hallucination. However, the processing time required for retrieval\n",
      "augmented large language models poses a challenge when applying them to tasks\n",
      "that require real-time responses, such as composition assistance.\n",
      "  To overcome this limitation, we propose the Hybrid Retrieval-Augmented\n",
      "Generation (HybridRAG) framework that leverages a hybrid setting that combines\n",
      "both client and cloud models. HybridRAG incorporates retrieval-augmented memory\n",
      "generated asynchronously by a Large Language Model (LLM) in the cloud. By\n",
      "integrating this retrieval augmented memory, the client model acquires the\n",
      "capability to generate highly effective responses, benefiting from the LLM's\n",
      "capabilities. Furthermore, through asynchronous memory integration, the client\n",
      "model is capable of delivering real-time responses to user requests without the\n",
      "need to wait for memory synchronization from the cloud. Our experiments on\n",
      "Wikitext and Pile subsets show that HybridRAG \u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mBased on the search results, \"Retrieval Augmented Generation\" refers to a technique that combines retrieval of information from external knowledge resources with text generation models. It has been applied in various natural language processing tasks, including dialogue response generation, machine translation, and composition assistance. The technique aims to enhance the contextual understanding of language models, reduce hallucination, and improve the quality of generated text. There are different approaches and methods proposed in the literature, such as retrieve-and-generate setups and active retrieval augmented generation. The effectiveness of retrieval augmented generation has been demonstrated in several studies, achieving state-of-the-art performance in various tasks.\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[32;1m\u001b[1;3mI should provide the final answer to the original input question.\n",
      "Final Answer: Retrieval Augmented Generation refers to a technique that combines retrieval of information from external knowledge resources with text generation models. It has been applied in various natural language processing tasks, including dialogue response generation, machine translation, and composition assistance. The technique aims to enhance the contextual understanding of language models, reduce hallucination, and improve the quality of generated text. There are different approaches and methods proposed in the literature, such as retrieve-and-generate setups and active retrieval augmented generation. The effectiveness of retrieval augmented generation has been demonstrated in several studies, achieving state-of-the-art performance in various tasks.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "CPU times: user 49.4 ms, sys: 1.48 ms, total: 50.9 ms\n",
      "Wall time: 421 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Retrieval Augmented Generation refers to a technique that combines retrieval of information from external knowledge resources with text generation models. It has been applied in various natural language processing tasks, including dialogue response generation, machine translation, and composition assistance. The technique aims to enhance the contextual understanding of language models, reduce hallucination, and improve the quality of generated text. There are different approaches and methods proposed in the literature, such as retrieve-and-generate setups and active retrieval augmented generation. The effectiveness of retrieval augmented generation has been demonstrated in several studies, achieving state-of-the-art performance in various tasks.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "agent_chain.run(\"What is Retrieval Augmented Generation?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right away, we can see that the chain only took ~0.3s, very promising! Let's check in WandB!\n",
    "\n",
    "This time, we cannot find information about token usage in Weights and Biases because we never actually needed to hit OpenAI's endpoint. \n",
    "\n",
    "Let's look at the Weights and Biases project output:\n",
    "\n",
    "![image](https://i.imgur.com/UjPsC6x.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see - we completely bypass the chain - and directly return the previous result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task\n",
    "\n",
    "Your task is to include both a prompt cache, and visibility to your application in any of your previous assignments, wrap it up in a Chainlit application, and host it on a Hugging Face Space (or EC2)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aims-visibility",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
